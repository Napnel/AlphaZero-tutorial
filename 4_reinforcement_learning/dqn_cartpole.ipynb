{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "from tensorflow.compat.v1.losses import huber_loss\n",
    "from utils.gpu_memory import limited_gpu_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory growth: True\n"
     ]
    }
   ],
   "source": [
    "limited_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v0'\n",
    "\n",
    "NUM_EPISODES = 500\n",
    "MAX_STEPS = 200\n",
    "GAMMA = 0.99\n",
    "\n",
    "\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(32, activation='relu', input_dim=num_states))\n",
    "        self.model.add(Dense(32, activation='relu'))\n",
    "        self.model.add(Dense(16, activation='relu'))\n",
    "        self.model.add(Dense(num_actions, activation='linear'))\n",
    "        \n",
    "        self.model.compile(loss=huber_loss, optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, memory_size):\n",
    "        self.buffer = deque(maxlen=memory_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
    "        return [self.buffer[i] for i in idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.main_qn = QNetwork(num_states, num_actions)\n",
    "        self.target_qn = QNetwork(num_states, num_actions)\n",
    "        print(self.main_qn.model.summary())\n",
    "        \n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        \n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        inputs = np.zeros((BATCH_SIZE, self.num_states))\n",
    "        targets = np.zeros((BATCH_SIZE, self.num_actions))\n",
    "\n",
    "        minibatch =self. memory.sample(BATCH_SIZE)\n",
    "\n",
    "        for i, (state_b, action_b, next_state_b, reward_b) in enumerate(minibatch):\n",
    "            inputs[i] = state_b\n",
    "\n",
    "            if not(next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
    "                target = reward_b + GAMMA * np.amax(self.target_qn.model.predict(next_state_b)[0])\n",
    "            else:\n",
    "                target = reward_b\n",
    "\n",
    "            targets[i] = self.main_qn.model.predict(state_b)\n",
    "            targets[i][action_b] = target\n",
    "\n",
    "        self.main_qn.model.fit(inputs, targets, epochs=1, verbose=0)\n",
    "        \n",
    "    def decide_action(self, state, episode):\n",
    "        epsilon = 0.5 * (1 / episode)\n",
    "        \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            return np.argmax(self.main_qn.model.predict(state)[0])\n",
    "        \n",
    "        return np.random.choice([0, 1])\n",
    "    \n",
    "    def update_target_q_network(self):\n",
    "        self.target_qn.model.set_weights(self.main_qn.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "        \n",
    "    def update_q_function(self):\n",
    "        self.brain.replay()\n",
    "        \n",
    "    def get_action(self, state, episode):\n",
    "        return self.brain.decide_action(state, episode)\n",
    "    \n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        self.brain.memory.push(state, action, state_next, reward)\n",
    "        \n",
    "    def get_action(self, state, episode):\n",
    "        return self.brain.decide_action(state, episode)\n",
    "        \n",
    "    def memorize(self, state, action, next_state, reward):\n",
    "        self.brain.memory.add((state, action, next_state, reward))\n",
    "        \n",
    "    def update_target_q_function(self):\n",
    "        self.brain.update_target_q_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enviroment:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)\n",
    "        self.num_states = self.env.observation_space.shape[0]\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.agent = Agent(self.num_states, self.num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        success_count = 0\n",
    "        episode_final = False\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "\n",
    "        for episode in range(1, NUM_EPISODES + 1):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, (1, self.num_states))\n",
    "            \n",
    "            for step in range(1, MAX_STEPS + 1):\n",
    "\n",
    "                # 行動決定\n",
    "                action = self.agent.get_action(state, episode)\n",
    "\n",
    "                next_state, _, done, _ = self.env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.num_states])\n",
    "\n",
    "                if done:\n",
    "                    if step >= 190:\n",
    "                        success_count += 1\n",
    "                        reward = 1\n",
    "                    else:\n",
    "                        success_count = 0 # 連続記録をリセット\n",
    "                        reward = -1\n",
    "\n",
    "                    next_state = np.zeros(next_state.shape)\n",
    "                    episode_10_list = np.hstack((episode_10_list[1:], step + 1))\n",
    "\n",
    "                else:\n",
    "                    reward = 0\n",
    "                \n",
    "                self.agent.memorize(state, action, next_state, reward)\n",
    "                \n",
    "                self.agent.update_q_function()\n",
    "                \n",
    "                state = next_state\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print(\"{} Episode: Finished after {} steps：10試行の平均step数 = {}\".format(episode, step, episode_10_list.mean()))\n",
    "                    if episode % 2 == 0:\n",
    "                        self.agent.update_target_q_function()\n",
    "                    break\n",
    "            \n",
    "            if episode_final is True:\n",
    "                self.env.render()\n",
    "                break\n",
    "\n",
    "            if success_count >= 5:\n",
    "                episode_final = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,778\n",
      "Trainable params: 1,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cartpole_env = Enviroment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.039709932306058016\n"
     ]
    }
   ],
   "source": [
    "# cartpole_env.env.action_space.sample()\n",
    "print(np.random.uniform(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Episode: Finished after 11 steps：10試行の平均step数 = 1.2\n",
      "2 Episode: Finished after 14 steps：10試行の平均step数 = 2.7\n",
      "3 Episode: Finished after 9 steps：10試行の平均step数 = 3.7\n",
      "4 Episode: Finished after 12 steps：10試行の平均step数 = 5.0\n",
      "5 Episode: Finished after 12 steps：10試行の平均step数 = 6.3\n",
      "6 Episode: Finished after 9 steps：10試行の平均step数 = 7.3\n",
      "7 Episode: Finished after 9 steps：10試行の平均step数 = 8.3\n",
      "8 Episode: Finished after 12 steps：10試行の平均step数 = 9.6\n",
      "9 Episode: Finished after 10 steps：10試行の平均step数 = 10.7\n",
      "10 Episode: Finished after 9 steps：10試行の平均step数 = 11.7\n",
      "11 Episode: Finished after 10 steps：10試行の平均step数 = 11.6\n",
      "12 Episode: Finished after 16 steps：10試行の平均step数 = 11.8\n",
      "13 Episode: Finished after 11 steps：10試行の平均step数 = 12.0\n",
      "14 Episode: Finished after 11 steps：10試行の平均step数 = 11.9\n",
      "15 Episode: Finished after 14 steps：10試行の平均step数 = 12.1\n",
      "16 Episode: Finished after 16 steps：10試行の平均step数 = 12.8\n",
      "17 Episode: Finished after 10 steps：10試行の平均step数 = 12.9\n",
      "18 Episode: Finished after 11 steps：10試行の平均step数 = 12.8\n",
      "19 Episode: Finished after 11 steps：10試行の平均step数 = 12.9\n",
      "20 Episode: Finished after 22 steps：10試行の平均step数 = 14.2\n",
      "21 Episode: Finished after 10 steps：10試行の平均step数 = 14.2\n",
      "22 Episode: Finished after 13 steps：10試行の平均step数 = 13.9\n",
      "23 Episode: Finished after 26 steps：10試行の平均step数 = 15.4\n",
      "24 Episode: Finished after 18 steps：10試行の平均step数 = 16.1\n",
      "25 Episode: Finished after 45 steps：10試行の平均step数 = 19.2\n",
      "26 Episode: Finished after 85 steps：10試行の平均step数 = 26.1\n",
      "27 Episode: Finished after 60 steps：10試行の平均step数 = 31.1\n",
      "28 Episode: Finished after 141 steps：10試行の平均step数 = 44.1\n",
      "29 Episode: Finished after 148 steps：10試行の平均step数 = 57.8\n",
      "30 Episode: Finished after 111 steps：10試行の平均step数 = 66.7\n"
     ]
    }
   ],
   "source": [
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('dl_env': conda)",
   "language": "python",
   "name": "python37664bitdlenvconda4ed1964df216439a8417d0a4f16ce08f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
